{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter0, Test Accuracy0.9231,Train Accuracy0.9191273\n",
      "Iter1, Test Accuracy0.9302,Train Accuracy0.9299091\n",
      "Iter2, Test Accuracy0.9397,Train Accuracy0.9401636\n",
      "Iter3, Test Accuracy0.9442,Train Accuracy0.94574547\n",
      "Iter4, Test Accuracy0.9479,Train Accuracy0.9499273\n",
      "Iter5, Test Accuracy0.9495,Train Accuracy0.9541636\n",
      "Iter6, Test Accuracy0.9494,Train Accuracy0.9561091\n",
      "Iter7, Test Accuracy0.9538,Train Accuracy0.9595091\n",
      "Iter8, Test Accuracy0.9564,Train Accuracy0.96187276\n",
      "Iter9, Test Accuracy0.9576,Train Accuracy0.96343637\n",
      "Iter10, Test Accuracy0.9583,Train Accuracy0.96567273\n",
      "Iter11, Test Accuracy0.9599,Train Accuracy0.9673455\n",
      "Iter12, Test Accuracy0.9609,Train Accuracy0.9683818\n",
      "Iter13, Test Accuracy0.9618,Train Accuracy0.96992725\n",
      "Iter14, Test Accuracy0.9643,Train Accuracy0.9708727\n",
      "Iter15, Test Accuracy0.9654,Train Accuracy0.9728182\n",
      "Iter16, Test Accuracy0.9653,Train Accuracy0.9730545\n",
      "Iter17, Test Accuracy0.9665,Train Accuracy0.9747273\n",
      "Iter18, Test Accuracy0.9665,Train Accuracy0.97516364\n",
      "Iter19, Test Accuracy0.9675,Train Accuracy0.9761636\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "y = tf.placeholder(\"float\", [None, 10])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# 建立一个简单的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop, W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10])+0.1)\n",
    "prediction = tf.nn.softmax(tf.matmul(L2_drop, W3) + b3)\n",
    "\n",
    "# 代价函数\n",
    "# （1）使用平方的方法\n",
    "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "# （2）使用交叉熵的方法改进 #########################################################\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))  # argmax 返回一维变量中最大值所在位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
    "        train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0})\n",
    "        print(\"Iter\" + str(epoch) + \", Test Accuracy\" + str(test_acc) + \",Train Accuracy\" + str(train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iter0, Test Accuracy0.9148,Train Accuracy0.9114\n",
    "Iter1, Test Accuracy0.9263,Train Accuracy0.9255091\n",
    "Iter2, Test Accuracy0.9323,Train Accuracy0.9324\n",
    "Iter3, Test Accuracy0.9344,Train Accuracy0.9366364\n",
    "Iter4, Test Accuracy0.9378,Train Accuracy0.9413091\n",
    "Iter5, Test Accuracy0.9413,Train Accuracy0.9453273\n",
    "Iter6, Test Accuracy0.9438,Train Accuracy0.9494\n",
    "Iter7, Test Accuracy0.9448,Train Accuracy0.9521818\n",
    "Iter8, Test Accuracy0.9475,Train Accuracy0.9530727\n",
    "Iter9, Test Accuracy0.9491,Train Accuracy0.95607275\n",
    "Iter10, Test Accuracy0.951,Train Accuracy0.9589273\n",
    "Iter11, Test Accuracy0.953,Train Accuracy0.9606364\n",
    "Iter12, Test Accuracy0.9532,Train Accuracy0.9622727\n",
    "Iter13, Test Accuracy0.9556,Train Accuracy0.96376365\n",
    "Iter14, Test Accuracy0.9563,Train Accuracy0.9652727\n",
    "Iter15, Test Accuracy0.9571,Train Accuracy0.967\n",
    "Iter16, Test Accuracy0.9584,Train Accuracy0.9685636\n",
    "Iter17, Test Accuracy0.96,Train Accuracy0.9693091\n",
    "Iter18, Test Accuracy0.9615,Train Accuracy0.9705273\n",
    "Iter19, Test Accuracy0.9614,Train Accuracy0.9717636\n",
    "\n",
    "2.使用dropout \n",
    "sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})\n",
    "Iter0, Test Accuracy0.9231,Train Accuracy0.9191273\n",
    "Iter1, Test Accuracy0.9302,Train Accuracy0.9299091\n",
    "Iter2, Test Accuracy0.9397,Train Accuracy0.9401636\n",
    "Iter3, Test Accuracy0.9442,Train Accuracy0.94574547\n",
    "Iter4, Test Accuracy0.9479,Train Accuracy0.9499273\n",
    "Iter5, Test Accuracy0.9495,Train Accuracy0.9541636\n",
    "Iter6, Test Accuracy0.9494,Train Accuracy0.9561091\n",
    "Iter7, Test Accuracy0.9538,Train Accuracy0.9595091\n",
    "Iter8, Test Accuracy0.9564,Train Accuracy0.96187276\n",
    "Iter9, Test Accuracy0.9576,Train Accuracy0.96343637\n",
    "Iter10, Test Accuracy0.9583,Train Accuracy0.96567273\n",
    "Iter11, Test Accuracy0.9599,Train Accuracy0.9673455\n",
    "Iter12, Test Accuracy0.9609,Train Accuracy0.9683818\n",
    "Iter13, Test Accuracy0.9618,Train Accuracy0.96992725\n",
    "Iter14, Test Accuracy0.9643,Train Accuracy0.9708727\n",
    "Iter15, Test Accuracy0.9654,Train Accuracy0.9728182\n",
    "Iter16, Test Accuracy0.9653,Train Accuracy0.9730545\n",
    "Iter17, Test Accuracy0.9665,Train Accuracy0.9747273\n",
    "Iter18, Test Accuracy0.9665,Train Accuracy0.97516364\n",
    "Iter19, Test Accuracy0.9675,Train Accuracy0.9761636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
